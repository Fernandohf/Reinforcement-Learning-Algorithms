{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2C Example\n",
    "\n",
    "This example shows how to use the Asyncronous TODO (A2C) Agent to solve an environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>GLOBAL:\tDEVICE:\tcuda\n",
       "\tSEED:\t42\n",
       "\tAGENT:\ta2c\n",
       "\tACTION_SIZE:\t1\n",
       "\tACTION_SPACE:\tcontinuous\n",
       "\tACTION_RANGE:\t[-2.0, 2.0]\n",
       "\tSTATE_SIZE:\t3\n",
       "\tENVIRONMENT:\tPendulum-v0\n",
       "\n",
       "A2C:\tACTOR:\tARCHITECTURE:\tfc\n",
       "\t\tHIDDEN_SIZE:\t[256, 32]\n",
       "\t\tLR:\t0.001\n",
       "\t\tWEIGHT_DECAY:\t0.0\n",
       "\t\tOPTIMIZER:\tadam\n",
       "\t\tHIDDEN_ACTIV:\trelu\n",
       "\t\tOUTPUT_LOC_ACTIV:\trelu\n",
       "\t\tOUTPUT_SCALE_ACTIV:\trelu\n",
       "\t\tOUTPUT_LOC_SCALER:\t1.0\n",
       "\n",
       "\tCRITIC:\tHIDDEN_SIZE:\t[256, 128]\n",
       "\t\tLR:\t0.001\n",
       "\t\tWEIGHT_DECAY:\t1e-05\n",
       "\t\tOPTIMIZER:\tadam\n",
       "\t\tARCHITECTURE:\tfc\n",
       "\n",
       "\tTRAINING:\tGAMMA:\t0.99\n",
       "\t\tN_STEP_BS:\t4\n",
       "\t\tLAMBDA:\t0.5\n",
       "\t\tGRADIENT_CLIP:\t0.0\n",
       "\n",
       "\n",
       "EXPLORATION:\tTYPE:\tgaussian\n",
       "\tSIZE:\t2\n",
       "\tEPS_BETA:\t0.1\n",
       "\tEPS_MIN:\t0.01\n",
       "\tMEAN:\t0.0\n",
       "\tSIGMA:\t0.4\n",
       "\tTHETA:\t0.01\n",
       "\n",
       "TRAINER:\tMAX_STEPS:\t200\n",
       "\tN_ENVS:\t50\n",
       "\tEPISODES:\t300\n",
       "\tPRINT_EVERY:\t50\n",
       "\tUPDATE_EVERY:\t10\n",
       "\tWANDB:\tFalse\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "ConfigObj({'GLOBAL': {'DEVICE': 'cuda', 'SEED': 42, 'AGENT': 'a2c', 'ACTION_SIZE': 1, 'ACTION_SPACE': 'continuous', 'ACTION_RANGE': [-2.0, 2.0], 'STATE_SIZE': 3, 'ENVIRONMENT': 'Pendulum-v0'}, 'A2C': {'ACTOR': {'ARCHITECTURE': 'fc', 'HIDDEN_SIZE': [256, 32], 'LR': 0.001, 'WEIGHT_DECAY': 0.0, 'OPTIMIZER': 'adam', 'HIDDEN_ACTIV': 'relu', 'OUTPUT_LOC_ACTIV': 'relu', 'OUTPUT_SCALE_ACTIV': 'relu', 'OUTPUT_LOC_SCALER': 1.0}, 'CRITIC': {'HIDDEN_SIZE': [256, 128], 'LR': 0.001, 'WEIGHT_DECAY': 1e-05, 'OPTIMIZER': 'adam', 'ARCHITECTURE': 'fc'}, 'TRAINING': {'GAMMA': 0.99, 'N_STEP_BS': 4, 'LAMBDA': 0.5, 'GRADIENT_CLIP': 0.0}}, 'EXPLORATION': {'TYPE': 'gaussian', 'SIZE': '2', 'EPS_BETA': 0.1, 'EPS_MIN': 0.01, 'MEAN': 0.0, 'SIGMA': 0.4, 'THETA': 0.01}, 'TRAINER': {'MAX_STEPS': 200, 'N_ENVS': 50, 'EPISODES': 300, 'PRINT_EVERY': 50, 'UPDATE_EVERY': 10, 'WANDB': False}})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bistrain.config.configuration import BisTrainConfiguration\n",
    "from bistrain.config import CONFIGSPEC_A2C\n",
    "# Define configuration file or Object\n",
    "config = BisTrainConfiguration('config_a2c.yaml', configspec=CONFIGSPEC_A2C)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "The trainer object loads all the necessary auxilary objects from the configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bistrain.trainer import Trainer\n",
    "# Trainer\n",
    "trainr = Trainer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b072f6cca0cb47358a40963ab721d55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=300), HTML(value='')), layout=Layout(display=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Miniconda\\lib\\site-packages\\gym\\envs\\classic_control\\pendulum.py:91: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return (((x+np.pi) % (2*np.pi)) - np.pi)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0699085e273b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\arquivos pessoais\\github\\reinforcement-learning-algorithms\\bistrain\\trainer.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, save)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMAX_STEPS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[0mscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mscores_deque\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mavg_scores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores_deque\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\arquivos pessoais\\github\\reinforcement-learning-algorithms\\bistrain\\agents.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, envs)\u001b[0m\n\u001b[0;32m     97\u001b[0m                                              \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                                              n_bootstrap=config.N_STEP_BS)\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_learn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;31m# Start from the next state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\arquivos pessoais\\github\\reinforcement-learning-algorithms\\bistrain\\agents.py\u001b[0m in \u001b[0;36m_learn\u001b[1;34m(self, states, actions, rewards, next_states, dones, gamma)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# Check consistency\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;31m# Current state, actions and next_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainr.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
